# -*- coding: utf-8 -*-
"""Context-Aware Chatbot Using LangChain or RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16P_QEyF1s0ndB52Lsivus_FHUPjJrFoh

# **Context-Aware Chatbot Using LangChain or RAG**
"""

pip install langchain==0.0.352

pip install langchain-community

pip install sentence-transformers

pip install faiss-cpu

pip install streamlit

pip install torch

pip install transformers

import os
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

# Ensure the 'data' directory exists
if not os.path.exists('data'):
    os.makedirs('data')

# Create a dummy docs.txt file if it doesn't exist
file_path = "data/docs.txt"
if not os.path.exists(file_path):
    with open(file_path, "w") as f:
        f.write("This is a sample document for testing purposes. It contains some text that will be split and embedded. "
                "LangChain is a framework for developing applications powered by language models. "
                "It enables applications that are context-aware and can reason.")

# Load dataset
loader = TextLoader(file_path)
documents = loader.load()

# Split text
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = splitter.split_documents(documents)

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

db = FAISS.from_documents(docs, embeddings)
db.save_local("faiss_db")

print("Vector database created")

import streamlit as st
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.llms import HuggingFacePipeline
from transformers import pipeline

st.set_page_config(page_title="RAG Chatbot")
st.title("ðŸ“š Context-Aware RAG Chatbot")

# Load embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Load vector store
db = FAISS.load_local(
    "faiss_db",
    embeddings
)


# Free LLM
hf_pipeline = pipeline(
    "text-generation",
    model="google/flan-t5-small",
    max_new_tokens=200
)
llm = HuggingFacePipeline(pipeline=hf_pipeline)

# Conversation memory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# RAG chain
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=db.as_retriever(),
    memory=memory
)

# UI
query = st.text_input("Ask a question:")
if query:
    answer = qa_chain.run(query)
    st.write("ðŸ¤– Bot:", answer)